

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>lncrnapy.modules &mdash; lncRNA-Py  documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../_static/doctools.js?v=9a2dae69"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="lncrnapy.train" href="train.html" />
    <link rel="prev" title="lncrnapy.features" href="features.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            lncRNA-Py
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../scripts.html">Running Scripts</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../api.html">API Documentation</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="data.html">lncrnapy.data</a></li>
<li class="toctree-l2"><a class="reference internal" href="features.html">lncrnapy.features</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">lncrnapy.modules</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#module-lncrnapy.modules.bert">BERT</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#lncrnapy.modules.bert.BERT"><code class="docutils literal notranslate"><span class="pre">BERT</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#lncrnapy.modules.bert.CSEBERT"><code class="docutils literal notranslate"><span class="pre">CSEBERT</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#lncrnapy.modules.bert.Decoder"><code class="docutils literal notranslate"><span class="pre">Decoder</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#lncrnapy.modules.bert.Encoder"><code class="docutils literal notranslate"><span class="pre">Encoder</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#lncrnapy.modules.bert.FeedForward"><code class="docutils literal notranslate"><span class="pre">FeedForward</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#lncrnapy.modules.bert.MultiHeadAttention"><code class="docutils literal notranslate"><span class="pre">MultiHeadAttention</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#lncrnapy.modules.bert.PositionalEncoding"><code class="docutils literal notranslate"><span class="pre">PositionalEncoding</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#lncrnapy.modules.bert.ResidualConnection"><code class="docutils literal notranslate"><span class="pre">ResidualConnection</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#lncrnapy.modules.bert.attention"><code class="docutils literal notranslate"><span class="pre">attention()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-lncrnapy.modules.cnn">CNN</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#lncrnapy.modules.cnn.CSEResNet"><code class="docutils literal notranslate"><span class="pre">CSEResNet</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#lncrnapy.modules.cnn.MycoAICNN"><code class="docutils literal notranslate"><span class="pre">MycoAICNN</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#lncrnapy.modules.cnn.ResNet"><code class="docutils literal notranslate"><span class="pre">ResNet</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#lncrnapy.modules.cnn.ResidualBlock"><code class="docutils literal notranslate"><span class="pre">ResidualBlock</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-lncrnapy.modules.conv_seq_encoding">Convolutional Sequence Encoding</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#lncrnapy.modules.conv_seq_encoding.ConvSeqEmbedding"><code class="docutils literal notranslate"><span class="pre">ConvSeqEmbedding</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#lncrnapy.modules.conv_seq_encoding.ConvSeqEncoding"><code class="docutils literal notranslate"><span class="pre">ConvSeqEncoding</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#module-lncrnapy.modules.wrappers">Wrappers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#lncrnapy.modules.wrappers.Classifier"><code class="docutils literal notranslate"><span class="pre">Classifier</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#lncrnapy.modules.wrappers.MaskedConvModel"><code class="docutils literal notranslate"><span class="pre">MaskedConvModel</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#lncrnapy.modules.wrappers.MaskedTokenModel"><code class="docutils literal notranslate"><span class="pre">MaskedTokenModel</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#lncrnapy.modules.wrappers.Regressor"><code class="docutils literal notranslate"><span class="pre">Regressor</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#lncrnapy.modules.wrappers.WrapperBase"><code class="docutils literal notranslate"><span class="pre">WrapperBase</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="train.html">lncrnapy.train</a></li>
<li class="toctree-l2"><a class="reference internal" href="evaluate.html">lncrnapy.evaluate</a></li>
<li class="toctree-l2"><a class="reference internal" href="utils.html">lncrnapy.utils</a></li>
<li class="toctree-l2"><a class="reference internal" href="algorithms.html">lncrnapy.algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="selection.html">lncrnapy.selection</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../experiments.html">Experiments</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">lncRNA-Py</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../api.html">API Documentation</a></li>
      <li class="breadcrumb-item active">lncrnapy.modules</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/luukromeijn/rhythmnblues/blob/master/docs/api_docs/modules.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="module-lncrnapy.modules">
<span id="lncrnapy-modules"></span><h1>lncrnapy.modules<a class="headerlink" href="#module-lncrnapy.modules" title="Link to this heading"></a></h1>
<section id="module-lncrnapy.modules.bert">
<span id="bert"></span><h2>BERT<a class="headerlink" href="#module-lncrnapy.modules.bert" title="Link to this heading"></a></h2>
<p>Contains base architectures (without output layers) of different neural
network designs.</p>
<dl class="py class">
<dt class="sig sig-object py" id="lncrnapy.modules.bert.BERT">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lncrnapy.modules.bert.</span></span><span class="sig-name descname"><span class="pre">BERT</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vocab_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">768</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">N</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">12</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_ff</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">h</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.bert.BERT" title="Link to this definition"></a></dt>
<dd><p>BERT base model, transformer encoder to be used for various tasks</p>
<p class="rubric">References</p>
<p>Transformer: Vaswani et al. (2017) <a class="reference external" href="https://doi.org/10.48550/arXiv.1706.03762">https://doi.org/10.48550/arXiv.1706.03762</a>
Code: Huang et al. (2022) <a class="reference external" href="https://nlp.seas.harvard.edu/annotated-transformer">https://nlp.seas.harvard.edu/annotated-transformer</a>
BERT: Devlin et al. (2019) <a class="reference external" href="https://doi.org/10.48550/arXiv.1810.04805">https://doi.org/10.48550/arXiv.1810.04805</a>
MycoAI: Romeijn et al. (2024) <a class="reference external" href="https://doi.org/10.1111/1755-0998.14006">https://doi.org/10.1111/1755-0998.14006</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="lncrnapy.modules.bert.BERT.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.bert.BERT.forward" title="Link to this definition"></a></dt>
<dd><p>Given a source, retrieve encoded representation</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lncrnapy.modules.bert.BERT.freeze">
<span class="sig-name descname"><span class="pre">freeze</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.bert.BERT.freeze" title="Link to this definition"></a></dt>
<dd><p>Freezes all weights of BERT model.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lncrnapy.modules.bert.BERT.unfreeze">
<span class="sig-name descname"><span class="pre">unfreeze</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.bert.BERT.unfreeze" title="Link to this definition"></a></dt>
<dd><p>Unfreezes all weights of BERT model.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lncrnapy.modules.bert.CSEBERT">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lncrnapy.modules.bert.</span></span><span class="sig-name descname"><span class="pre">CSEBERT</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_kernels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">768</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">N</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">12</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_ff</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">h</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_linear</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_relu</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_hidden_kernels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.bert.CSEBERT" title="Link to this definition"></a></dt>
<dd><p>BERT variant that takes learnt convolutional sequence encodings (instead
of tokens) as input. Based on vision transformer.</p>
<p class="rubric">References</p>
<p>ViT: Dosovitskiy et al. (2020) <a class="reference external" href="https://doi.org/10.48550/arXiv.2010.11929">https://doi.org/10.48550/arXiv.2010.11929</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="lncrnapy.modules.bert.CSEBERT.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.bert.CSEBERT.forward" title="Link to this definition"></a></dt>
<dd><p>Given a source, retrieve encoded representation</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lncrnapy.modules.bert.CSEBERT.freeze">
<span class="sig-name descname"><span class="pre">freeze</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.bert.CSEBERT.freeze" title="Link to this definition"></a></dt>
<dd><p>Freezes all weights of CSEBERT model.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lncrnapy.modules.bert.CSEBERT.freeze_kernels">
<span class="sig-name descname"><span class="pre">freeze_kernels</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.bert.CSEBERT.freeze_kernels" title="Link to this definition"></a></dt>
<dd><p>Freezes all CSE weights of CSEBERT model.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lncrnapy.modules.bert.CSEBERT.unfreeze">
<span class="sig-name descname"><span class="pre">unfreeze</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.bert.CSEBERT.unfreeze" title="Link to this definition"></a></dt>
<dd><p>Unfreezes all weights of CSEBERT model.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lncrnapy.modules.bert.Decoder">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lncrnapy.modules.bert.</span></span><span class="sig-name descname"><span class="pre">Decoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_ff</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">h</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">N</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">self_attention</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.bert.Decoder" title="Link to this definition"></a></dt>
<dd><p>N layers of consisting of (masked) (self-)attention and FF sublayers,
gradually transforms encoder’s output and output embedding into decoding</p>
<dl class="py method">
<dt class="sig sig-object py" id="lncrnapy.modules.bert.Decoder.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">m</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_mask</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tgt_mask</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.bert.Decoder.forward" title="Link to this definition"></a></dt>
<dd><p>Pass the input (and mask) through each layer in turn.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lncrnapy.modules.bert.Encoder">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lncrnapy.modules.bert.</span></span><span class="sig-name descname"><span class="pre">Encoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_ff</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">h</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">N</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.bert.Encoder" title="Link to this definition"></a></dt>
<dd><p>N layers of consisting of self-attention and feed forward sublayers,
gradually transforms input into encoded representation.</p>
<dl class="py method">
<dt class="sig sig-object py" id="lncrnapy.modules.bert.Encoder.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.bert.Encoder.forward" title="Link to this definition"></a></dt>
<dd><p>Pass the input (and mask) through each layer in turn.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lncrnapy.modules.bert.FeedForward">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lncrnapy.modules.bert.</span></span><span class="sig-name descname"><span class="pre">FeedForward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_ff</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.bert.FeedForward" title="Link to this definition"></a></dt>
<dd><p>Simple feed forward network (with dropout applied to mid layer)</p>
<dl class="py method">
<dt class="sig sig-object py" id="lncrnapy.modules.bert.FeedForward.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.bert.FeedForward.forward" title="Link to this definition"></a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lncrnapy.modules.bert.MultiHeadAttention">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lncrnapy.modules.bert.</span></span><span class="sig-name descname"><span class="pre">MultiHeadAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">h</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.bert.MultiHeadAttention" title="Link to this definition"></a></dt>
<dd><p>‘Performs scaled dot product attention on h uniquely learned linear
projections (allowing model to attend to info from different subspaces)</p>
<dl class="py method">
<dt class="sig sig-object py" id="lncrnapy.modules.bert.MultiHeadAttention.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.bert.MultiHeadAttention.forward" title="Link to this definition"></a></dt>
<dd><p>Implements Figure 2 from ‘Attention Is All You Need’</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lncrnapy.modules.bert.PositionalEncoding">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lncrnapy.modules.bert.</span></span><span class="sig-name descname"><span class="pre">PositionalEncoding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5000</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.bert.PositionalEncoding" title="Link to this definition"></a></dt>
<dd><p>Adds positional information to an inputted embedding.</p>
<dl class="py method">
<dt class="sig sig-object py" id="lncrnapy.modules.bert.PositionalEncoding.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.bert.PositionalEncoding.forward" title="Link to this definition"></a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lncrnapy.modules.bert.ResidualConnection">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lncrnapy.modules.bert.</span></span><span class="sig-name descname"><span class="pre">ResidualConnection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.bert.ResidualConnection" title="Link to this definition"></a></dt>
<dd><p>Employs a normalized residual connection followed by dropout</p>
<dl class="py method">
<dt class="sig sig-object py" id="lncrnapy.modules.bert.ResidualConnection.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.bert.ResidualConnection.forward" title="Link to this definition"></a></dt>
<dd><p>Adds layer(x) to x and applies normalization/dropout</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="lncrnapy.modules.bert.attention">
<span class="sig-prename descclassname"><span class="pre">lncrnapy.modules.bert.</span></span><span class="sig-name descname"><span class="pre">attention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.bert.attention" title="Link to this definition"></a></dt>
<dd><p>Compute ‘Scaled Dot Product Attention’</p>
</dd></dl>

</section>
<section id="module-lncrnapy.modules.cnn">
<span id="cnn"></span><h2>CNN<a class="headerlink" href="#module-lncrnapy.modules.cnn" title="Link to this heading"></a></h2>
<p>CNN architectures that can possibly be used as base architectures in
<cite>lncrnapy</cite>.</p>
<dl class="py class">
<dt class="sig sig-object py" id="lncrnapy.modules.cnn.CSEResNet">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lncrnapy.modules.cnn.</span></span><span class="sig-name descname"><span class="pre">CSEResNet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_kernels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layers</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.cnn.CSEResNet" title="Link to this definition"></a></dt>
<dd><p>Like ResNet, but initial layers correspond to CSE.</p>
<dl class="py method">
<dt class="sig sig-object py" id="lncrnapy.modules.cnn.CSEResNet.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.cnn.CSEResNet.forward" title="Link to this definition"></a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lncrnapy.modules.cnn.MycoAICNN">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lncrnapy.modules.cnn.</span></span><span class="sig-name descname"><span class="pre">MycoAICNN</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">conv_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[5,</span> <span class="pre">10]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">in_channels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pool_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_normalization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.cnn.MycoAICNN" title="Link to this definition"></a></dt>
<dd><p>A simple CNN architecture with conv, batchnorm and maxpool layers, as
used by MycoAI-CNN.</p>
<p class="rubric">References</p>
<p>MycoAI: Romeijn et al. (2024) <a class="reference external" href="https://doi.org/10.1111/1755-0998.14006">https://doi.org/10.1111/1755-0998.14006</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="lncrnapy.modules.cnn.MycoAICNN.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.cnn.MycoAICNN.forward" title="Link to this definition"></a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lncrnapy.modules.cnn.ResNet">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lncrnapy.modules.cnn.</span></span><span class="sig-name descname"><span class="pre">ResNet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">in_channels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.cnn.ResNet" title="Link to this definition"></a></dt>
<dd><p>Adapted from:
<a class="reference external" href="https://blog.paperspace.com/writing-resnet-from-scratch-in-pytorch/">https://blog.paperspace.com/writing-resnet-from-scratch-in-pytorch/</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="lncrnapy.modules.cnn.ResNet.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.cnn.ResNet.forward" title="Link to this definition"></a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lncrnapy.modules.cnn.ResidualBlock">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lncrnapy.modules.cnn.</span></span><span class="sig-name descname"><span class="pre">ResidualBlock</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">downsample</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.cnn.ResidualBlock" title="Link to this definition"></a></dt>
<dd><p>Adapted from:
<a class="reference external" href="https://blog.paperspace.com/writing-resnet-from-scratch-in-pytorch/">https://blog.paperspace.com/writing-resnet-from-scratch-in-pytorch/</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="lncrnapy.modules.cnn.ResidualBlock.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.cnn.ResidualBlock.forward" title="Link to this definition"></a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="module-lncrnapy.modules.conv_seq_encoding">
<span id="convolutional-sequence-encoding"></span><h2>Convolutional Sequence Encoding<a class="headerlink" href="#module-lncrnapy.modules.conv_seq_encoding" title="Link to this heading"></a></h2>
<p>Contains modules related to convolutional sequence encoding, which uses a
simple 1D convolutional neural network to extract kernels from input sequences.
This is similar to how Vision Transformers (ViTs) work.</p>
<p class="rubric">References</p>
<p>ViT: Dosovitskiy et al. (2020) <a class="reference external" href="https://doi.org/10.48550/arXiv.2010.11929">https://doi.org/10.48550/arXiv.2010.11929</a></p>
<dl class="py class">
<dt class="sig sig-object py" id="lncrnapy.modules.conv_seq_encoding.ConvSeqEmbedding">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lncrnapy.modules.conv_seq_encoding.</span></span><span class="sig-name descname"><span class="pre">ConvSeqEmbedding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_kernels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_linear</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_relu</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_hidden_kernels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.conv_seq_encoding.ConvSeqEmbedding" title="Link to this definition"></a></dt>
<dd><p>Projects convolutional sequence encoding into space of pre-specified
dimensionality.</p>
<dl class="py method">
<dt class="sig sig-object py" id="lncrnapy.modules.conv_seq_encoding.ConvSeqEmbedding.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.conv_seq_encoding.ConvSeqEmbedding.forward" title="Link to this definition"></a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lncrnapy.modules.conv_seq_encoding.ConvSeqEncoding">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lncrnapy.modules.conv_seq_encoding.</span></span><span class="sig-name descname"><span class="pre">ConvSeqEncoding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_kernels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_relu</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_hidden_kernels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.conv_seq_encoding.ConvSeqEncoding" title="Link to this definition"></a></dt>
<dd><p>Implementation for convolutional sequence encoding using a small 1D CNN.</p>
<dl class="py method">
<dt class="sig sig-object py" id="lncrnapy.modules.conv_seq_encoding.ConvSeqEncoding.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.conv_seq_encoding.ConvSeqEncoding.forward" title="Link to this definition"></a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lncrnapy.modules.conv_seq_encoding.ConvSeqEncoding.visualize">
<span class="sig-name descname"><span class="pre">visualize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filepath</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.conv_seq_encoding.ConvSeqEncoding.visualize" title="Link to this definition"></a></dt>
<dd><p>Visualizes a certain kernel, indicated by <cite>kernel_idx</cite>. In case of
hidden kernels, will visualize the first kernel layer (= hidden).</p>
</dd></dl>

</dd></dl>

</section>
<section id="module-lncrnapy.modules.wrappers">
<span id="wrappers"></span><h2>Wrappers<a class="headerlink" href="#module-lncrnapy.modules.wrappers" title="Link to this heading"></a></h2>
<p>Contains wrapper classes that enhance a base architecture (which can be any
PyTorch module) with additional requirements for various (pre-)training tasks
from <cite>lncrnapy</cite>.</p>
<dl class="py class">
<dt class="sig sig-object py" id="lncrnapy.modules.wrappers.Classifier">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lncrnapy.modules.wrappers.</span></span><span class="sig-name descname"><span class="pre">Classifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">base_arch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pooling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'CLS'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pred_batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.wrappers.Classifier" title="Link to this definition"></a></dt>
<dd><p>Wrapper class that uses a base architecture to perform binary
classification.</p>
<dl class="py method">
<dt class="sig sig-object py" id="lncrnapy.modules.wrappers.Classifier.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_logits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.wrappers.Classifier.forward" title="Link to this definition"></a></dt>
<dd><p>A forward pass through the neural network.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lncrnapy.modules.wrappers.Classifier.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_logits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.wrappers.Classifier.predict" title="Link to this definition"></a></dt>
<dd><p>Calls <cite>forward</cite> in batch-wise fashion for all rows in <cite>data</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<cite>lncrnapy.data.Data</cite>) – Data object with <cite>tensor_features</cite> attribute.</p></li>
<li><p><strong>**kwargs</strong> – Any keyword argument accepted by the model’s forward method.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lncrnapy.modules.wrappers.MaskedConvModel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lncrnapy.modules.wrappers.</span></span><span class="sig-name descname"><span class="pre">MaskedConvModel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">base_arch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_hidden_kernels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_linear</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_relu</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pred_batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.wrappers.MaskedConvModel" title="Link to this definition"></a></dt>
<dd><p>Wrapper class for model that performs Masked Language Modeling with
cse-encoded sequences as input.</p>
<dl class="py method">
<dt class="sig sig-object py" id="lncrnapy.modules.wrappers.MaskedConvModel.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.wrappers.MaskedConvModel.forward" title="Link to this definition"></a></dt>
<dd><p>A forward pass through the neural network.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lncrnapy.modules.wrappers.MaskedTokenModel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lncrnapy.modules.wrappers.</span></span><span class="sig-name descname"><span class="pre">MaskedTokenModel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">base_arch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pred_batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.wrappers.MaskedTokenModel" title="Link to this definition"></a></dt>
<dd><p>Wrapper class for model that performs Masked Language Modeling with
tokenized sequences as input.</p>
<dl class="py method">
<dt class="sig sig-object py" id="lncrnapy.modules.wrappers.MaskedTokenModel.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.wrappers.MaskedTokenModel.forward" title="Link to this definition"></a></dt>
<dd><p>A forward pass through the neural network.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lncrnapy.modules.wrappers.Regressor">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lncrnapy.modules.wrappers.</span></span><span class="sig-name descname"><span class="pre">Regressor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">base_arch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fcn_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pred_batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pooling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'CLS'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.wrappers.Regressor" title="Link to this definition"></a></dt>
<dd><p>Wrapper class for model that performs linear regression on the base
architecture embedding.</p>
<dl class="py method">
<dt class="sig sig-object py" id="lncrnapy.modules.wrappers.Regressor.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.wrappers.Regressor.forward" title="Link to this definition"></a></dt>
<dd><p>A forward pass through the neural network.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lncrnapy.modules.wrappers.WrapperBase">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lncrnapy.modules.wrappers.</span></span><span class="sig-name descname"><span class="pre">WrapperBase</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">base_arch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pred_batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.wrappers.WrapperBase" title="Link to this definition"></a></dt>
<dd><p>Base class for all wrapper modules in <cite>lncrnapy</cite>.</p>
<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">`base_arch`</span></span></dt>
<dd><p>PyTorch module to be used as base architecture of the classifier.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><cite>torch.nn.Module</cite></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">`pred_batch_size`</span></span></dt>
<dd><p>Batch size used by the <cite>predict</cite> method.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><cite>int</cite></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">`data_columns`</span></span></dt>
<dd><p>Data column name for outcome of predict method.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><cite>list</cite> | <cite>str</cite></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">`latent_space_columns`</span></span></dt>
<dd><p>Data column name for latent space columns (only defined after calling
<cite>latent_space</cite> method.)</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p><cite>list</cite></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lncrnapy.modules.wrappers.WrapperBase.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.wrappers.WrapperBase.forward" title="Link to this definition"></a></dt>
<dd><p>A forward pass through the neural network.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lncrnapy.modules.wrappers.WrapperBase.latent_space">
<span class="sig-name descname"><span class="pre">latent_space</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pooling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim_red</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">TSNE()</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.wrappers.WrapperBase.latent_space" title="Link to this definition"></a></dt>
<dd><p>Calculates latent representation for all rows in <cite>data</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<cite>lncrnapy.data.Data</cite>) – Data object for which latent space should be calculated.</p></li>
<li><p><strong>inplace</strong> (<cite>bool</cite>) – If True, adds latent space as feature columns to <cite>data</cite>.</p></li>
<li><p><strong>pooling</strong> (<em>[</em><em>'CLS'</em><em>, </em><em>'max'</em><em>, </em><em>'mean'</em><em>, </em><em>None</em><em>]</em>) – How to aggregate token embeddings (for BERT architectures).
* ‘CLS’: use only CLS token.
* ‘max’: max pooling over (non-padding) token embeddings.
* ‘mean’: mean pooling over (non-padding) token embeddings.
* None (default): no pooling, e.g. for CNN base architectures.</p></li>
<li><p><strong>dim_red</strong> (<cite>sklearn</cite> | <cite>NoneType</cite>) – Dimensionality reduction algorithm from <cite>sklearn</cite> to use.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lncrnapy.modules.wrappers.WrapperBase.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inplace</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lncrnapy.modules.wrappers.WrapperBase.predict" title="Link to this definition"></a></dt>
<dd><p>Calls <cite>forward</cite> in batch-wise fashion for all rows in <cite>data</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<cite>lncrnapy.data.Data</cite>) – Data object with <cite>tensor_features</cite> attribute.</p></li>
<li><p><strong>**kwargs</strong> – Any keyword argument accepted by the model’s forward method.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="features.html" class="btn btn-neutral float-left" title="lncrnapy.features" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="train.html" class="btn btn-neutral float-right" title="lncrnapy.train" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Luuk Romeijn.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>