{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LncRNA classification with an RNA language model\n",
    "This example notebook shows the basic functionalities of the `lncrnapy`\n",
    "package and how it can be used to train an RNA language model for lncRNA \n",
    "classification. \n",
    "\n",
    "## Data\n",
    "Let us start by loading some sequence data. The `Data` object accepts either a \n",
    "single fasta file (for unlabelled data), or a list of two fasta files (for pcRNA\n",
    "and lncRNA, respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data...\n",
      "Imported 297724 protein-coding and 238470 non-coding RNA transcripts with 0 feature(s).\n",
      "                                                       id  \\\n",
      "0       ENST00000676272.1|ENSG00000087053.20|OTTHUMG00...   \n",
      "1       ENST00000676132.1|ENSG00000133424.22|OTTHUMG00...   \n",
      "2       ENST00000504953.5|ENSG00000196104.11|OTTHUMG00...   \n",
      "3       ENST00000677479.1|ENSG00000168610.17|OTTHUMG00...   \n",
      "4       ENST00000513066.3|ENSG00000113407.14|OTTHUMG00...   \n",
      "...                                                   ...   \n",
      "536189                                        NR_026711.1   \n",
      "536190                                        NR_189643.1   \n",
      "536191                                        NR_026710.1   \n",
      "536192                                        NR_027231.1   \n",
      "536193                                        NR_130733.1   \n",
      "\n",
      "                                                 sequence  label  \n",
      "0       AGCCTACAGGCGCGGTGCACTCTGGGGGAACATGGCCGCTTCCGGT...  pcRNA  \n",
      "1       AAGGATCCTCATGGCAGCATGGAGAGGCCAGGCGTGTCTGCGTGGC...  pcRNA  \n",
      "2       TCCTCTGGCTCCAGCCAAGCGTCCTATCCGGAGCCAACTGTAGCTG...  pcRNA  \n",
      "3       GCAGACTGGGAGGGGGAGCCGGGGGTTCCGACGTCGCAGCCGAGGG...  pcRNA  \n",
      "4       TCCCGGGCGCTAGCCCACCTCCCACCCGCCTCTTGGCTCCTCTCCT...  pcRNA  \n",
      "...                                                   ...    ...  \n",
      "536189  TTGTTTTCTTTTATTTATTTATTTAGACGGAGTCTCACTCTGTCGC...  ncRNA  \n",
      "536190  CTAGGAGCCAAGACCTCTTGCTGGCTGCCACATCGGCACTGCCAAC...  ncRNA  \n",
      "536191  TTGTTTTCTTTTATTTATTTATTTAGACGGAGTCTCACTCTGTCGC...  ncRNA  \n",
      "536192  GGGACCGGGCCGCTTCTTGTTCGGCGTGGGCGGCTCCTGGCAGACC...  ncRNA  \n",
      "536193  AAGCATTTGGCAAGCACATCCTGGGTGCCAGGCCCAAGCAGGCTGG...  ncRNA  \n",
      "\n",
      "[536194 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from lncrnapy.data import Data\n",
    "\n",
    "data_dir = '/data/s2592800/data' # Change this for your setup\n",
    "\n",
    "pretrain_data = Data([f'{data_dir}/sequences/pretrain_human_pcrna.fasta',\n",
    "                      f'{data_dir}/sequences/pretrain_human_ncrna.fasta'])\n",
    "\n",
    "print(pretrain_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "Next, we encode the data into a numeric Tensor format that is compatible with \n",
    "the neural network that we wish to use. For Motif Encoding, we must encode the \n",
    "data into a four-dimensional representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_data.set_tensor_features('4D-DNA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate what we just did, let us sample a sequence by indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence:\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Label (0=ncRNA, 1=pcRNA):\n",
      "tensor([1.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "sequence, label = pretrain_data[0] # Sample the first sequence\n",
    "print(\"Sequence:\")\n",
    "print(sequence)\n",
    "print(\"Label (0=ncRNA, 1=pcRNA):\")\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural architecture\n",
    "Now we can define a model. The most important component of our model is its \n",
    "base architecture. `lncrnapy.modules` contains several types of\n",
    "architectures implementations, including CNNs (e.g. `ResNet`) and language \n",
    "models (e.g. `BERT`). Motif Encoding requires a special variant of BERT, which \n",
    "is implemented in the `MotifBERT` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lncrnapy.modules import MotifBERT\n",
    "base_arch = MotifBERT(n_motifs=4096, motif_size=9) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lncrnapy.modules` also contains several wrapper classes that encapsulate a\n",
    "base architecture and add the required layers to perform tasks like \n",
    "classification, regression, and masked language modeling. For example, this is\n",
    "how we turn our model into a classifier: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lncrnapy.modules import Classifier\n",
    "from lncrnapy import utils\n",
    "\n",
    "model = Classifier(base_arch)\n",
    "model = model.to(utils.DEVICE) # Send the model to the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a prediction on the validation dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data...\n",
      "Imported 5583 protein-coding and 2998 non-coding RNA transcripts with 0 feature(s).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4755],\n",
      "        [0.4519],\n",
      "        [0.4753],\n",
      "        ...,\n",
      "        [0.4651],\n",
      "        [0.4455],\n",
      "        [0.4553]])\n"
     ]
    }
   ],
   "source": [
    "# Load and encode the data\n",
    "valid_data = Data([f'{data_dir}/sequences/valid_gencode_pcrna.fasta',\n",
    "                   f'{data_dir}/sequences/valid_gencode_ncrna.fasta'])\n",
    "valid_data.set_tensor_features('4D-DNA')\n",
    "\n",
    "# Make a prediction\n",
    "prediction = model.predict(valid_data)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training\n",
    "The prediction made above is only a random one, as we have not trained our model\n",
    "yet. Language models are often pre-trained before being fine-tuned to perform \n",
    "a specific task. We shall do the same. \n",
    "\n",
    "First, we must wrap the base architecture into the proper wrapper class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lncrnapy.modules import MaskedMotifModel\n",
    "\n",
    "model = MaskedMotifModel(base_arch).to(utils.DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train it using the `train_masked_motif_modeling` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lncrnapy.train import train_masked_motif_modeling\n",
    "\n",
    "model, history = train_masked_motif_modeling(\n",
    "    model, pretrain_data, valid_data, epochs=500\n",
    ")\n",
    "\n",
    "print(history) # Contains the performance at every epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning\n",
    "After pre-training, we can extract the base architecture and wrap it inside a\n",
    "`Classifier` object again. We can then fine-tune our model using the \n",
    "`train_classifier` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lncrnapy.train import train_classifier\n",
    "\n",
    "finetune_data = Data([f'{data_dir}/sequences/finetune_gencode_pcrna.fasta',\n",
    "                      f'{data_dir}/sequences/finetune_gencode_ncrna.fasta'])\n",
    "\n",
    "model = Classifier(model.base_arch).to(utils.DEVICE)\n",
    "model, history = train_classifier(model, pretrain_data, valid_data, epochs=100)\n",
    "\n",
    "print(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lorna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
